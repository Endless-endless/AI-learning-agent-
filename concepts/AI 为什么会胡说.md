
AI 会“一本正经地胡说”，并不是因为它故意出错，而是因为其工作机制决定了它的输出目标是“看起来最合理”，而不是真实或正确。

大语言模型生成文本时，只是在预测下一个最可能出现的词，并不具备事实核验或自我怀疑的能力。

当问题本身模糊、上下文不足，或涉及模型未充分学习的领域时，AI 会倾向于自行补全，从而产生看似合理但实际错误的内容。

当前理解层级：能力边界理解

相关概念：
- [[大语言模型]]
- [[提示词]]

关联：[[Day2_AI_knowledge_structure]]